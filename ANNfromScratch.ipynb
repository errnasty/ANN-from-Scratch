{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN from scratch \n",
    "We will be making an Artificial Neural Network without any libraries such as tensorflow and pytorch. The only library we will be using will be numpy and pandas. \n",
    "\n",
    "What better to learn about ANNs than to do it yourself, without using any of the fancy libraries and getting it done with just a few lines of ann.add to get a full Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Used\n",
    "We will be using the famous MNIST dataset. Which is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n",
    "\n",
    "![](./images/MnistExamplesModified.png)\n",
    "\n",
    "__Sources__\n",
    "- http://yann.lecun.com/exdb/mnist/\n",
    "- https://www.wikiwand.com/en/MNIST_database\n",
    "- https://www.kaggle.com/competitions/digit-recognizer/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [7, 0, 0, ..., 0, 0, 0],\n",
       "       [6, 0, 0, ..., 0, 0, 0],\n",
       "       [9, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(df)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "We want our final product to be inputed a 28x28 image of a handwritten digit and for it to predict from 0 to 9. \n",
    "\n",
    "![](./images/Screenshot%202023-10-04%20182409.png)\n",
    "\n",
    "\n",
    "### How are we going to tackle this problem?\n",
    "Through Artificial Neural Networks! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Artificial Neural Network?\n",
    "It is a model inspried by the structure and function of neurons in the human brain. Which in our case takes in 784 pixels as our input for the first layer of our ANN and using some maths and magic, we get a output of what the machine thinks the image.\n",
    "\n",
    "![](./images/Screenshot%202023-10-05%20150956.png)\n",
    "\n",
    "We can think of each neuron in the network as just a number between 0 - 1. Where the neuron lights up when its value is close to 1 and remains inactive when near 0. We can represent each neuron as an activation function $a_n$ where n is the placement of the neuron along each layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH\n",
    "Together with the connections between each neuron in the input layer and 1 neuron of the 2nd hidden layer (784 connections), we can get a formula of $w_1a_1 + w_2a_2 + w_3a_3 ... w_na_n$ where we get the weighted sum of all the activations. \n",
    "\n",
    "When we compute a weighted sum like this, but for this network we want the activation for the neuron to be somewhere between 0 - 1. A common thing to do is to put this weighted sum in a some function that squishes the real number into the range between 0 and 1. We will be using a sigmoid function or a RELU function to squish our formula. \n",
    "\n",
    "$$σ(w_1a_1 + w_2a_2 + w_3a_3 ... w_na_n)$$\n",
    "\n",
    "What if you want for some bias for the neuron to be inactive, we'll just add in the number into the formula before we apply the sigmoid squishfycation function.\n",
    "\n",
    "$$σ(w_1a_1 + w_2a_2 + w_3a_3 ... w_na_n + b)$$\n",
    "\n",
    "So far each neuron from the input layer has to be connected to each neuron in the other hidden layers and then they have to be connected to the output layer, this makes our network to have a total of 13,002 total weights and bias to tweak and optimise!!\n",
    "\n",
    "To get the math equation for the full transition of activations from 1 layer to the next. \n",
    "\n",
    "We organise all the activations from the first layer into a column as a vector. \n",
    "$$\\begin{bmatrix} a_0(0) \\\\ a_1(0) \\\\ ... \\\\ a_n(0) \\end{bmatrix}$$\n",
    "\n",
    "Then organise the weights as a matrix where each row corresponds to the connections between one layer and a particular neuron in the next layer. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} w_0,0 &  w_0,1 & \\cdots & w_0,n \\\\ w_1,0 &  w_1,1 & \\cdots & w_1,n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_k,0 &  w_k,1 & \\cdots & w_k,n \\end{bmatrix}\n",
    "\\begin{bmatrix} a_0(0) \\\\ a_1(0) \\\\ \\vdots \\\\ a_n(0) \\end{bmatrix} = \n",
    "\\begin{bmatrix} ? \\\\ ? \\\\ \\vdots \\\\ ? \\end{bmatrix}\n",
    "$$\n",
    "Where we get the product of each neuron\n",
    "\n",
    "Next we add the bias, where we organise it into a vector and adding it to our matrix vector product.\n",
    "$$\n",
    "\\begin{bmatrix} w_0,0 &  w_0,1 & \\cdots & w_0,n \\\\ w_1,0 &  w_1,1 & \\cdots & w_1,n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_k,0 &  w_k,1 & \\cdots & w_k,n \\end{bmatrix}\n",
    "\\begin{bmatrix} a_0(0) \\\\ a_1(0) \\\\ \\vdots \\\\ a_n(0) \\end{bmatrix} + \n",
    "\\begin{bmatrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally we squish each component from our result\n",
    "$$\n",
    "σ\\left(\n",
    "\\begin{bmatrix} w_0,0 &  w_0,1 & \\cdots & w_0,n \\\\ w_1,0 &  w_1,1 & \\cdots & w_1,n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_k,0 &  w_k,1 & \\cdots & w_k,n \\end{bmatrix}\n",
    "\\begin{bmatrix} a_0(0) \\\\ a_1(0) \\\\ \\vdots \\\\ a_n(0) \\end{bmatrix} + \n",
    "\\begin{bmatrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_n \\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "We can represent this formula with the matrixs as their own symbols \n",
    "$$\n",
    "a(1) = σ(Wa(0) + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the Network learn? and the MORE Math\n",
    "To understand how a neural network learns, let's dive deeper into the concepts of cost functions, gradient descent, and backpropagation.\n",
    "\n",
    "### Cost Function\n",
    "So how do we know how bad or good our model is performing on a given dataset? We use a cost function, which is a mathematical function that measures how well or poorly a neural network is performing on the data. In our case we add up the squares of the differences between the unwanted output activations and the value we want them to have in the output layer. \n",
    "\n",
    "For example, we will be using a picture of 3 as our input. The ANN spits out trash outputs but we want it to predict a 3. \n",
    "$$\n",
    "(0.43 - 0.00)^2 +(0.23 - 0.00)^2 +(0.54 - 0.00)^2 +\\color{green}{(0.88 - 1.00)^2} \\color{white}+(0.54 - 0.00)^2 +(0.02 - 0.00)^2 +(0.25 - 0.00)^2 +(0.12 - 0.00)^2 +(0.77 - 0.00)^2 +(0.63 - 0.00)^2\n",
    "$$\n",
    "We get a result of 2.3002. This higher the cost function the shitter the model is, with 0 being the perfect prediction.\n",
    "### Gradient Descent\n",
    "But just knowing how shit the model is, isnt very helpful. We need to tell it how to change the weights and bias so that it gets better aka reduce the cost function. Thats where gradient descent comes in.\n",
    "\n",
    "The idea of gradient descent is to eventually minimize the cost function by taking calculated steps in the direction of the gradient.\n",
    "![](./images/gradientdescent.png)\n",
    "[Source](https://sebastianraschka.com/faq/docs/gradient-optimization.html)\n",
    "\n",
    "We first take the derititive(gradient) of our cost function $C()$ and when it is positve we move to the left and right if it is negative. And if we do this repeatly at each step checking the gradient and taking the right step, we will eventually reach a local minimum of the function. We can think of it as a ball rolling down the hill. However unlike our example above, there can be many local minimums in a function and depending on where we start it can converge into different local minimums.\n",
    "\n",
    "If we make our step size __propotional__ to the gradient calculated, we will take big steps when the gradient is nowhere close to 0 and much smaller and careful steps when it is nearing the local minimum.\n",
    "$$\\theta_{i+1} = \\theta_i - \\alpha \\nabla J(\\theta_i)$$\n",
    "- $\\theta_i$ represents the current values of the parameters (weights or coefficients).\n",
    "- $\\alpha$ is the learning rate, which determines the step size or how much you adjust the parameters in each iteration.\n",
    "- $\\nabla J(\\theta_i)$ represents the gradient of the cost function with respect to the parameters at the current point \\theta_i.\n",
    "\n",
    "### Back Propagation\n",
    "Back propagation is the process using gradient descent and using it to minimize the cost function, by updating our 13,002 weights and bias of the neural network. The goal is to understand how a change in each weight and bias impacts the cost function.\n",
    "\n",
    "As it names suggest, we move backwards in the neural network, looking first at the output of our network. The main idea is to find each desired changes to the weights and bias that best fit our data.\n",
    "\n",
    "The main steps are:\n",
    "- __Backward Pass__: The gradient of the cost function is calculated with respect to each parameter in the neural network. This is typically done using the chain rule of calculus(we will get to it later). The gradients indicate how much the cost function would change if the parameters were adjusted slightly or how important each parameter is to the overall cost function.\n",
    "\n",
    "- __Weight and Bias Updates__: The gradients calculated in the backward pass are used to update the weights and biases of the neural network. The parameters are adjusted in the __opposite direction__ of the gradient to minimize the cost. The learning rate is a hyperparameter that determines the size of the step taken during each update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Backpropagation math\n",
    "\n",
    "Lets begin by having a simple neural network where each layer has a single neuron on it\n",
    "\n",
    "![](./images/Screenshot%202023-10-13%20155200.png)\n",
    "\n",
    "This simple network has 3 weights and 3 bias, and our goal is the understand how __senstitve__ the cost function is to these variables. \n",
    "$$C(w_1,b_1,w_2,b_2,w_3,b_3)$$\n",
    "\n",
    "In that way we know which adjustments to the variables is going to cause the most efficient decrease to the cost function.\n",
    "\n",
    "First we look at the connection between the last 2 neurons,lets label the last neuron $a^{(L)}$, indicating the layer that is in and the 2nd last neuron $a^{(L-1)}$\n",
    "\n",
    "![](./images/Screenshot%202023-10-13%20160730.png)\n",
    "\n",
    "Lets say the value we want this last activation to be for a given traning example is $\\color{yellow}y$. \n",
    "\n",
    "![](./images/Screenshot%202023-10-13%20162053.png)\n",
    "\n",
    "The cost $C_0(...)$ of this for a single training exmaple is $(a^L-\\color{yellow}y)^2$. For example $(0.66-1.00)^2$\n",
    "\n",
    "Remeber that our activation is equals to the weight times the activation of the previous layer plus the bias with a function to squish it in between 0 and 1, we can write it such that $\\color{gray}a^{(L)} = σ(\\color{blue}w^{(L)}a^{(L-1)} + \\color{purple}b^{(L)})$\n",
    "\n",
    "It will be easier if give a special name to the weighted sum like $z^{(L)}$ with the same superscript as the relevant activations.:\n",
    "$$\\color{green}z^{(L)} = \\color{blue}w^{(L)}a^{(L-1)} + \\color{purple}b^{(L)}$$\n",
    "$$\\color{gray}a^{(L)} = \\color{green}z^{(L)}$$\n",
    "\n",
    "A way to visualize this is to visualize it to a flow chart. Where the weight, previous activation and bias are all used to compute $z^{(L)}$, which in turn lets us compute $a^{(L)}$, and lastly with the constant $y$ lets us compute the cost:\n",
    "\n",
    "![](./images/Screenshot%202023-10-13%20163800.png)\n",
    "\n",
    "And $a^{(L)}$ is also influenced by its own weights and bias from the previous layer\n",
    "\n",
    "![](./images/Screenshot%202023-10-13%20165255.png)\n",
    "But lets ignore $a^{(L)}$ for now\n",
    "\n",
    "Our first small goal is to understand how sensitive the cost function $\\color{red}C_0$ is to small changes in our weight $\\color{blue}w^{(L)}$. Or in other words what the derivative of $\\color{red}C_0(...)$ with respect to $\\color{blue}w^{(L)}$. What we want is the ratio, the nudge to $\\color{blue}w^{(L)}$ causes some nudge to $\\color{green}z^{(L)}$, which in turn changes $\\color{gray}a^{(L)}$, which directly influences the cost $\\color{red}C_0$\n",
    "\n",
    "So we break it up by looking first at the ratio of a change to $\\color{green}z^{(L)}$ to the change in $\\color{blue}w^{(L)}$. Which is the derivative of $\\color{green}z^{(L)}$ with respect to $\\color{blue}w^{(L)}$. Then we consider the chage of $a^{(L)}$ to the change in $\\color{green}z^{(L)}$ as well as the final change in $\\color{red}C_0$ and $\\color{gray}a^{(L)}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}C_0}{\\color{blue}w^{(L)}} = \\frac{\\color{green}z^{(L)}}{\\color{blue}w^{(L)}} \\cdot \\frac{\\color{gray}a^{(L)}}{\\color{green}z^{(L)}} \\cdot \\frac{\\color{red}C_0}{\\color{gray}a^{(L)}}\n",
    "$$\n",
    "\n",
    "This formula here is the chain rule!\n",
    "\n",
    "The final result will be:\n",
    "$$\n",
    "\\frac{\\color{red}C_0}{\\color{blue}w^{(L)}} = \\frac{\\color{green}z^{(L)}}{\\color{blue}w^{(L)}} \\cdot \\frac{\\color{gray}a^{(L)}}{\\color{green}z^{(L)}} \\cdot \\frac{\\color{red}C_0}{\\color{gray}a^{(L)}} = a^{(L-1)}σ'(z^{L})2(a^{(L)}-y)\n",
    "$$\n",
    "\n",
    "However this is only the derivative with respect to $\\color{blue}w^{(L)}$ to the cost of a single training example, as the cost function requries us to average throughout all the training examples. We need to take the average of all the derivatives of each training example :\n",
    "$$\n",
    "\\frac{\\color{red}C}{\\color{blue}w^{(L)}} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\color{red}C_k}{\\color{blue}w^{(L)}}\n",
    "$$\n",
    "\n",
    "From our final result, we already have 50% of the work done! If we want to find how does $\\color{purple}b^{(L)}$ change in respect to $\\color{red}C_0$, we can just replace $\\color{blue}w^{(L)}$ with $\\color{purple}b^{(L)}$. And our result will be:\n",
    "$$\n",
    "\\frac{\\color{red}C_0}{\\color{purple}b^{(L)}} = \\frac{\\color{green}z^{(L)}}{\\color{purple}b^{(L)}} \\cdot \\frac{\\color{gray}a^{(L)}}{\\color{green}z^{(L)}} \\cdot \\frac{\\color{red}C_0}{\\color{gray}a^{(L)}} = 1σ'(z^{L})2(a^{(L)}-y)\n",
    "$$\n",
    "\n",
    "Also this is the idea where propagating backwards comes in, we can see how sensitve the cost function is the to previous activation layer $a^{(L-1)}$\n",
    "$$\n",
    "\\frac{\\color{red}C_0}{a^{(L-1)}} = \\frac{\\color{green}z^{(L)}}{a^{(L-1)}} \\cdot \\frac{\\color{gray}a^{(L)}}{\\color{green}z^{(L)}} \\cdot \\frac{\\color{red}C_0}{\\color{gray}a^{(L)}} = \\color{blue}w^{(L)}\\color{white}σ'(z^{L})2(a^{(L)}-y)\n",
    "$$\n",
    "Because now we can just kepp iterating the chain rule backwards to see how sensitve the cost function is to the previous weights and bias!\n",
    "\n",
    "#### How about a network with multiple neurons in a single layer?\n",
    "It is actually not that complicated, there are just a few more indices to keep track of. Rather than a layer having the label $a^{(L)}$ we are also going to have a subscript indicating which neuron of that layer it is. Lets use the letter $k$ to index the layer $a^{(L-1)}$ and $j$ to index the layer $a^{(L)}$\n",
    "\n",
    "Similiarly to find the cost function we add up the sum of our desired outputs.$\\color{red}C_0 = \\color{white}\\sum_{j=1}^{n_L-1} (a_j^{(L)}- y_j)^2$ \n",
    "\n",
    "The chain rule equation is essentially the same, but with different indicies:\n",
    "$$\n",
    "\\frac{\\color{red}C_0}{\\color{blue}w_{jk}^{(L)}} = \\frac{\\color{green}z_{j}^{(L)}}{\\color{blue}w_{jk}^{(L)}} \\cdot \\frac{\\color{gray}a_{j}^{(L)}}{\\color{green}z_{j}^{(L)}} \\cdot \\frac{\\color{red}C_0}{\\color{gray}a_{j}^{(L)}}\n",
    "$$\n",
    "where the weight connecting the $a_{k}^{(L-1)}$ neuron and $a_{j}^{(L)}$ is $\\color{blue}w_{jk}^{(L)}$\n",
    "\n",
    "What changes is the derivatives of the cost with respect to the one of the activations of the previous layer. The difference is that the activation of the neuron influences the cost function through mulitple different paths. So we need to find the sum of the derivatives of layer L:\n",
    "$$\n",
    "\\frac{\\color{red}C_0}{a_{k}^{(L-1)}} = \\sum_{j=1}^{n_L-1} \\frac{\\color{green}z_{j}^{(L)}}{a_{k}^{(L-1)}} \\cdot \\frac{\\color{gray}a_{j}^{(L)}}{\\color{green}z_{j}^{(L)}} \\cdot \\frac{\\color{red}C_0}{\\color{gray}a_{j}^{(L)}}\n",
    "$$\n",
    "Once we know how sensitve the cost function is, we can just repeat it with the rest of th weights and bias throughout the layer. \n",
    "\n",
    "We are done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [9 6 0 ... 3 4 2]\n",
      "(784,) (784, 8400)\n"
     ]
    }
   ],
   "source": [
    "m ,n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:, 1:].T\n",
    "Y = data[:, 0].T\n",
    "\n",
    "print(X, Y)\n",
    "\n",
    "x_train = X[:, :int(0.8*m)]\n",
    "y_train = Y[:int(0.8*m)]\n",
    "\n",
    "x_test = X[:, int(0.8*m):]\n",
    "y_test = Y[int(0.8*m):]\n",
    "\n",
    "print(x_train[:,0].shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def inital_params():\n",
    "    w1 = np.random.randn(16, 784)\n",
    "    b1 = np.random.randn(16, 1)\n",
    "    w2 = np.random.randn(16, 16)\n",
    "    b2 = np.random.randn(16, 1)\n",
    "    w3 = np.random.randn(10, 16)\n",
    "    b3 = np.random.randn(10, 1)\n",
    "\n",
    "    return w1, b1, w2, b2, w3, b3\n",
    "\n",
    "# def ReLu(z):\n",
    "#     return np.maximum(z, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z), axis=0)\n",
    "\n",
    "def forware_propagation(w1, b1, w2, b2, w3, b3, X):\n",
    "    z1 = np.dot(w1, X) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(w3, a2) + b3\n",
    "    a3 = softmax(z3)\n",
    "\n",
    "    return z1, a1, z2, a2, z3, a3\n",
    "\n",
    "def encode_y(y):\n",
    "    y_encode = np.zeros((y.size, y.max()+1))\n",
    "    y_encode[np.arange(y.size), y] = 1\n",
    "\n",
    "    return y_encode\n",
    "\n",
    "def cost_function(a3, y):\n",
    "    m = y.shape[0]\n",
    "    cost = (-1/m) * np.sum(y * np.log(a3) + (1-y) * np.log(1-a3))\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "# def back_propagation(w1, b1, w2, b2, w3, b3, X, Y):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
